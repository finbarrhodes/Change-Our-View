{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf525069-56da-4c90-a95f-a23a868cd671",
   "metadata": {
    "id": "bf525069-56da-4c90-a95f-a23a868cd671"
   },
   "source": [
    "# ChangeMyView - Big Data Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5rDJIB8LQa27",
   "metadata": {
    "id": "5rDJIB8LQa27"
   },
   "source": [
    "##  Project Pipeline — Notebook-by-Notebook Roadmap  \n",
    "\n",
    "| Stage | Notebook | Main Goal | Key Actions | Output written to GCS |\n",
    "|-------|----------|-----------|-------------|-----------------------|\n",
    "| **N1 · Data Acquisition & Pre-processing** | `Notebook_01_Data_preprocessing.ipynb` | Turn 4.3 GB of raw CMV JSON into a clean, tokenised dataset. | 1. Download Zenodo dump (65 k threads).<br>2. Keep only *original* posts (OPs).<br>3. Merge `title` + `selftext`; regex scrub; NLTK tokenise & lemmatise.<br>4. Persist cleaned DF partitioned by `thread_id`. | `gs://cmv-bucket/n1_preprocessed_df/` |\n",
    "| **N2 · Topic Discovery (LDA)** | `Notebook_02_LDA.ipynb` | Uncover discussion themes. | 1. Load N1 DF.<br>2. `CountVectorizer` → sparse TF matrix.<br>3. Fit 45-topic Spark LDA; manual coherence tuning.<br>4. Map topics → 14 macro-categories; attach dominant topic to each post. | `gs://cmv-bucket/n2_categorised_df/` |\n",
    "| **N3 · Feature Engineering** | `Notebook_03_Feature_engineering.ipynb` | Build a rich feature set for persuasion prediction. | 1. Load N2 DF.<br>2. Add lexical, structural, interaction features.<br>3. Standardise & assemble into a `features` vector. | `gs://cmv-bucket/n3_features_df/` |\n",
    "| **N4 · Persuasion Modelling** | `Notebook_04_Modelling.ipynb` | Predict whether a comment earns a Δ. | 1. Load N3 DF.<br>2. Handle class imbalance (SMOTE + undersampling).<br>3. Train & tune Logistic Reg., RF, GBT, Dist. Trees.<br>4. Log metrics (F<sub>1</sub>, precision, recall). | `gs://cmv-bucket/n4_models/` |\n",
    "| **N5 · Exploratory Visualisation** | `Notebook_05_Visualisation.ipynb` | Tell the story visually. | 1. Load N3 (or N4) artefacts.<br>2. Plot topic prevalence, Δ-rates, t-SNE clusters, time trends. | Figures saved to `gs://cmv-bucket/figures/` |\n",
    "\n",
    "> **Dependency chain:** N1 ➜ N2 ➜ N3 ➜ {N4, N5}.  \n",
    "> Each notebook can be rerun independently as long as its upstream artefact folder exists in GCS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2UqgonmbQ1n5",
   "metadata": {
    "id": "2UqgonmbQ1n5"
   },
   "source": [
    "# Notebook 01 – Data Pre-processing Pipeline\n",
    "*A first look at how we turn 4.3 GB of raw ChangeMyView JSON into tidy, model-ready tables.*\n",
    "\n",
    "> **Goal in one line:** build a fully distributed Spark pipeline that cleans, labels, and tokenises each discussion thread **without pulling any data back to the driver**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f5e94-a345-42db-bccc-734dea116816",
   "metadata": {
    "id": "f72f5e94-a345-42db-bccc-734dea116816"
   },
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51354a-44fe-4bd9-8dcb-bc577c4ff3b3",
   "metadata": {
    "id": "bd51354a-44fe-4bd9-8dcb-bc577c4ff3b3",
    "outputId": "3e716d34-a1c7-4ad0-98bf-713299e073a6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType,  LongType\n",
    "from pyspark.sql.functions import from_unixtime, year, month, date_format\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "import re\n",
    "from pyspark.sql.functions import split\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pyspark.sql.functions import udf, concat_ws, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "english_words = set(words.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710f88f-52ca-482d-b912-6430378314f0",
   "metadata": {
    "id": "7710f88f-52ca-482d-b912-6430378314f0"
   },
   "source": [
    "\n",
    "## 1 - Data source  \n",
    "\n",
    "[https://zenodo.org/records/1043504]\n",
    "\n",
    "**Explicit Schema**\n",
    "\n",
    "To speed up loading and avoid schema inference errors, we define an explicit `StructType` for the Reddit JSON.  \n",
    "This guarantees consistent data types across runs and avoids surprises like `LongType` misinterpretations or null-handling issues.\n",
    "\n",
    "There is a description of key fields included in the code below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199184b-f818-4eaa-90e2-ceff385927da",
   "metadata": {
    "id": "d199184b-f818-4eaa-90e2-ceff385927da",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema_two = StructType([\n",
    "    StructField(\"num_comments\", IntegerType(), True),     # number of comments in the post\n",
    "    StructField(\"selftext\", StringType(), True),          # descriptive text of the post\n",
    "    StructField(\"score\", IntegerType(), True),            # ?????????\n",
    "    StructField(\"title\", StringType(), True),             # title of the post\n",
    "    StructField(\"delta\", BooleanType(), True),            # target\n",
    "    StructField(\"urls\", ArrayType(StringType()), True),   # number of urls in the post\n",
    "    StructField(\"name\", StringType(), True),              # user unique identifier\n",
    "    StructField(\"created_utc\", IntegerType(), True)       # date and time of creation\n",
    "])\n",
    "\n",
    "df_change = spark.read.schema(schema_two).json(\"gs://st446-cmv/threads.jsonl.bz2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YnSdtNaMSOSc",
   "metadata": {
    "id": "YnSdtNaMSOSc"
   },
   "source": [
    "## 2 - Data preprocessing\n",
    "\n",
    "1. **Timestamp fix-up**\n",
    "2. **Moderator cleanup**\n",
    "3. **Field merge**\n",
    "4. **Regex scrub**\n",
    "5. **Tokenisation & lemmatisation**\n",
    "6. **Token split**\n",
    "\n",
    "Sample output:\n",
    "- `merged`: \"I think this should be changed because...\"\n",
    "- `processed`: \\[think, changed\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063d668-3550-4288-baa0-75d33a3d3290",
   "metadata": {
    "id": "0063d668-3550-4288-baa0-75d33a3d3290",
    "outputId": "12241bec-1f75-4068-d35e-caf86b5914b4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (65169, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#data check for the shape\n",
    "shape_two = (df_change.count(), len(df_change.columns))\n",
    "print(f\"Shape: {shape_two}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diNpt6RtSfeX",
   "metadata": {
    "id": "diNpt6RtSfeX"
   },
   "source": [
    "1. **Timestamp fix-up** – cast `created_utc` → Spark `timestamp`, add `year_month` for partitioning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d0b73-5295-42c9-af62-a2d0da8078e0",
   "metadata": {
    "id": "644d0b73-5295-42c9-af62-a2d0da8078e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Timestamp fix-up\n",
    "# Convert the createdUTC to an understandable date time\n",
    "df_change = df_change.withColumn(\"created_datetime\", from_unixtime(\"created_utc\").cast(\"timestamp\"))\n",
    "df_change = df_change.withColumn(\"year_month\", date_format(\"created_datetime\", \"yyyy-MM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ccb4d1-bd6e-4f9e-b81d-88ff1f79ee76",
   "metadata": {
    "id": "c5ccb4d1-bd6e-4f9e-b81d-88ff1f79ee76",
    "outputId": "116a6b35-39b8-47a0-a341-c51c08c4bd90",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           min_date|           max_date|\n",
      "+-------------------+-------------------+\n",
      "|2013-01-17 16:24:11|2017-09-30 23:13:07|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display time range\n",
    "df_change.agg(\n",
    "    min(\"created_datetime\").alias(\"min_date\"),\n",
    "    max(\"created_datetime\").alias(\"max_date\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5568c-9625-498e-8057-de21e34904cc",
   "metadata": {
    "id": "1bf5568c-9625-498e-8057-de21e34904cc"
   },
   "source": [
    "2. **Remove moderator comments** – some post contain moderator comments in `selftext` in between stars\n",
    "\n",
    "Notice that not all posts contain 'selftext', meaning all information is contained in the title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830b1fb",
   "metadata": {
    "collapsed": true,
    "id": "c830b1fb",
    "outputId": "b81d4a5f-7686-4b88-b330-f4ed9dd288ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|selftext                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|I visited Toronto in 2012 and was extremely impressed. There's tons of stuff to do there, it's very vibrant, it's booming, and for a city of its size, it's actually very affordable to live in.\\n\\nIt's like what Portland wants to be, but 5 times the size. The only issue I have with Toronto is the people are not particularly friendly, but the same could be said of almost any city.\\n\\nThe food is excellent as well, and I love the waterfront and all the parks. Even though it's a huge city, you never feel that far away from nature.\\n\\nIt's also in Canada, which is my favorite country in the world. Does anyone want to try to change my mind and suggest a better city on this continent?\\n\\n_____\\n\\n&gt;  .     .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|This year I was enrolled in three AP courses: US History, Language &amp; Composition, and Biology. In my experience, the volume of information taught in AP classes is undermined by what a passionless trek the curriculum is. Each of these courses had a very strict curriculum involving large amounts of rote memorization, along with rehearsing the same long answer techniques and structures over and over. I think this makes students less invested in the subject and more likely to forget everything they learned the second the test is over.\\n\\nCompare this to an Honors or a CP (standard-level) course, where teachers are able to design lessons to fit the needs of their class. They can assign projects that relate to student interests and conduct lessons in a way that makes them passionate, which can contribute to an effective learning environment. Getting students invested can make them more likely to be invested in later classes in related subjects, or even to pursue a career in that field. \\n\\nThe flexibility also allows teachers to cater to different learning styles and performance levels. Even at an AP level, people learn differently. Students would be much more engaged with the lessons - much more likely to learn - if their individual needs were met.\\n\\nPersonally, I think I learned more in standard level classes. I can still remember individual lessons and interesting projects, and I can apply that knowledge to my life. Reading it out of an Amsco textbook just doesn't cut it, for me and for a lot of people I've spoken to.\\n\\nHopefully somebody here can convince me I didn't just waste a year of my life. Other than college credit, what is the value of taking AP courses?\\n\\n_____\\n\\n&gt;  .     . |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Remove some moderator comments at the bottom of some comments that were obstructing future process\n",
    "def remove_starred_text(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    # All occurence between ** are taken out\n",
    "    return re.sub(r\"\\*{1,3}.*?\\*{1,3}\", \"\", text)\n",
    "\n",
    "remove_starred_text_udf = udf(remove_starred_text, StringType())\n",
    "df_removed = df_change.withColumn(\"selftext\", remove_starred_text_udf(col(\"selftext\")))\n",
    "df_removed.select(\"selftext\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kUudKj0ZXoq5",
   "metadata": {
    "id": "kUudKj0ZXoq5"
   },
   "source": [
    "3. **Merge fields** – concatenate `title` + `selftext` into a single `merged` column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d7d78-b87d-4c18-8633-c33b6befb272",
   "metadata": {
    "id": "494d7d78-b87d-4c18-8633-c33b6befb272",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we merge text + title\n",
    "df_merged = df_removed.withColumn(\"merged\", concat_ws(\" \", col(\"selftext\"), col(\"title\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "njhX8Bb5X-Xb",
   "metadata": {
    "id": "njhX8Bb5X-Xb"
   },
   "source": [
    "4. **Regex scrub** – lower-case, strip markdown, punctuation, and URLs with `regexp_replace`.  \n",
    "5. **Tokenise + lemmatise** – NLTK `word_tokenize` + WordNet in a UDF; extended stop-list removes CMV-specific filler (“change”, “view”, “opinion”…).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u2uuSZYBX8OZ",
   "metadata": {
    "id": "u2uuSZYBX8OZ"
   },
   "outputs": [],
   "source": [
    "# Specific list of words with high repetition on CMV 'titles' or  'selftext'\n",
    "custom_stopwords = set(stopwords.words('english')).union({\n",
    "    'also', 'could', 'one', 'would', 'use', 'say', 'even', 'thing', 'get', 'much',\n",
    "    'change', 'view', 'opinion', 'really', 'make', 'still', 'see', 'think',\n",
    "    'know', 'like', 'way', 'go', 'come', 'want','think','believe', 'wa', 'ha', 'dont', 'people',\n",
    "    'feel','', 'please', 'u', 'people', 'dont', 'please', 'feel', 'first', 'comment', 'question', 'read', 'look', 'effective', 'concern',\n",
    "    'right', 'message', 'report', 'speaking','however','going','footnote','thinking','topic','firstly','many','moderator','user','hello',\n",
    "    'something','cant','someone'})\n",
    "\n",
    "# We preprocess carefully the full text as best as possible for lda\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in custom_stopwords]\n",
    "    tokens = [word for word in tokens if word in english_words]\n",
    "    return \" \".join(tokens)\n",
    "preprocess_udf = udf(preprocess_text, StringType())\n",
    "df_processed = df_merged.withColumn(\"processed\", preprocess_udf(col(\"merged\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YkYrhugoZdjA",
   "metadata": {
    "id": "YkYrhugoZdjA"
   },
   "source": [
    "6. **Final Token Split**\n",
    "\n",
    "At this point, the `processed` column still contains each post as a single space-separated string of lemmatised, cleaned words.  \n",
    "We now convert this into an actual array of tokens using Spark's `split()` function, which will be required for downstream steps.\n",
    "\n",
    "The output below shows:\n",
    "- `merged`: the raw text (title + selftext)\n",
    "- `processed`: a list of cleaned tokens, with stopwords and punctuation already removed\n",
    "\n",
    "This transformation ensures our pipeline remains distributed and avoids flattening the token list into separate rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca8684",
   "metadata": {
    "id": "bfca8684",
    "outputId": "aec37827-7acb-4d0f-af13-bd7c9833209a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------------------------------------------------------------------------------------------------------------------------------------+-----+----+---------+-----------+-------------------+----------+---------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+\n",
      "|num_comments|selftext|score|title                                                                                                                                 |delta|urls|name     |created_utc|created_datetime   |year_month|merged                                                                                                                                 |processed                                                     |\n",
      "+------------+--------+-----+--------------------------------------------------------------------------------------------------------------------------------------+-----+----+---------+-----------+-------------------+----------+---------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+\n",
      "|16          |        |1    |CMV: I don't believe there is any good reason to ever feel angry about anything. I don't think it can ever help you even a little bit.|false|[]  |t3_3i1y4z|NULL       |NULL               |NULL      | CMV: I don't believe there is any good reason to ever feel angry about anything. I don't think it can ever help you even a little bit.|[good, reason, ever, angry, anything, ever, help, little, bit]|\n",
      "|1           |        |1    |I Believe Junior Soldiers Are Grossly Overpaid                                                                                        |false|[]  |t3_24tiws|NULL       |NULL               |NULL      | I Believe Junior Soldiers Are Grossly Overpaid                                                                                        |[junior, soldier, grossly]                                    |\n",
      "|2           |        |1    |I think they should \"cultivate\" a more laid back culture here cmv                                                                     |false|[]  |NULL     |1369635147 |2013-05-27 06:12:27|2013-05   | I think they should \"cultivate\" a more laid back culture here cmv                                                                     |[cultivate, laid, back, culture]                              |\n",
      "+------------+--------+-----+--------------------------------------------------------------------------------------------------------------------------------------+-----+----+---------+-----------+-------------------+----------+---------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#split processed\n",
    "final_df = df_processed.withColumn(\"processed\", split(df_processed[\"processed\"], \" \"))\n",
    "final_df.show(3, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sVMgN2mvfhHV",
   "metadata": {
    "id": "sVMgN2mvfhHV"
   },
   "source": [
    "## 3 – Export to Cloud Storage  \n",
    "\n",
    "We write the final cleaned DataFrame to GCS as JSON, partitioned by `thread_id` to preserve debate structure.  \n",
    "This output will be used as input for topic modelling in Notebook 02.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880af908-ac5b-4119-aeb7-a1a4b849ae24",
   "metadata": {
    "id": "880af908-ac5b-4119-aeb7-a1a4b849ae24",
    "outputId": "9a7f393b-19c2-439f-8005-390696ef06fb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==============================================>           (4 + 1) / 5]\r"
     ]
    }
   ],
   "source": [
    "final_df.write \\\n",
    "         .mode(\"overwrite\") \\\n",
    "         .option(\"header\", \"true\") \\\n",
    "         .json(\"gs://st446-cmv/n1_preprocessing_df/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35426335-250c-4b43-a776-63a51fb09fe5",
   "metadata": {
    "id": "35426335-250c-4b43-a776-63a51fb09fe5"
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2f4b1-53f0-4c39-a53d-ed0ce98ce1b3",
   "metadata": {
    "id": "2bd2f4b1-53f0-4c39-a53d-ed0ce98ce1b3"
   },
   "source": [
    "### Just politics & technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303fcbe3",
   "metadata": {
    "id": "303fcbe3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_topic(text: str) -> int:\n",
    "    politics_keywords = {\n",
    "        \"government\", \"policy\", \"election\", \"senate\", \"congress\", \"democracy\", \"republic\", \"president\",\n",
    "        \"left-wing\", \"right-wing\", \"liberal\", \"conservative\", \"capitalism\", \"socialism\", \"nationalism\",\n",
    "        \"politician\", \"parliament\", \"vote\", \"voting\", \"law\", \"legislation\", \"public sector\", \"minister\",\n",
    "        \"trump\", \"tax\", \"policy\",\"policy\",\"democrat\",\"republican\"\n",
    "    }\n",
    "\n",
    "    technology_keywords = {\n",
    "        \"ai\", \"artificial intelligence\", \"black box\", \"cpu\", \"machine learning\", \"technology\", \"software\", \"hardware\",\n",
    "        \"robotics\", \"data\", \"algorithm\", \"python\", \"coding\", \"developer\", \"engineer\", \"app\", \"computer\",\n",
    "        \"neural network\", \"gpu\", \"chip\", \"tech\", \"smartphone\", \"device\", \"programming\", \"tech industry\"\n",
    "    }\n",
    "\n",
    "    # Compile regex once per executor (works well for large datasets)\n",
    "    politics_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(word) for word in politics_keywords) + r')\\b', flags=re.IGNORECASE)\n",
    "    technology_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(word) for word in technology_keywords) + r')\\b', flags=re.IGNORECASE)\n",
    "\n",
    "    text = text.lower() if isinstance(text, str) else \"\"\n",
    "\n",
    "    politics_match = bool(politics_pattern.search(text))\n",
    "    technology_match = bool(technology_pattern.search(text))\n",
    "\n",
    "    if politics_match and technology_match:\n",
    "        return 2  # Other\n",
    "    elif politics_match:\n",
    "        return 0  # Politics\n",
    "    elif technology_match:\n",
    "        return 1  # Technology\n",
    "    else:\n",
    "        return 2  # Other\n",
    "\n",
    "classify_udf = udf(classify_topic, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf29794-b6d5-44d0-af46-c88e2a08d260",
   "metadata": {
    "id": "4bf29794-b6d5-44d0-af46-c88e2a08d260",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#politics or technology\n",
    "df_change = df_change.withColumn(\"explicit_class\", classify_udf(\"processed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658250ce-3655-46d0-af02-c73013dbff84",
   "metadata": {
    "collapsed": true,
    "id": "658250ce-3655-46d0-af02-c73013dbff84",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "04c9ac30-78e5-4cec-a064-f60dbcca337c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/19 10:58:54 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 311) (st446-cluster-w08-w-0.europe-west2-c.c.delta-container-448310-n4.internal executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12453/2792152010.py\", line 6, in preprocess_text\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 129, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 106, in sent_tokenize\n",
      "    tokenizer = PunktTokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/data.py\", line 582, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/home/nltk_data'\n",
      "    - '/opt/conda/miniconda3/nltk_data'\n",
      "    - '/opt/conda/miniconda3/share/nltk_data'\n",
      "    - '/opt/conda/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:181)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/04/19 10:59:05 WARN TaskSetManager: Lost task 1.1 in stage 14.0 (TID 314) (st446-cluster-w08-w-0.europe-west2-c.c.delta-container-448310-n4.internal executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12453/2792152010.py\", line 6, in preprocess_text\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 129, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 106, in sent_tokenize\n",
      "    tokenizer = PunktTokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/data.py\", line 582, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/home/nltk_data'\n",
      "    - '/opt/conda/miniconda3/nltk_data'\n",
      "    - '/opt/conda/miniconda3/share/nltk_data'\n",
      "    - '/opt/conda/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:181)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/04/19 10:59:16 WARN TaskSetManager: Lost task 0.2 in stage 14.0 (TID 315) (st446-cluster-w08-w-1.europe-west2-c.c.delta-container-448310-n4.internal executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12453/2792152010.py\", line 6, in preprocess_text\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 129, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 106, in sent_tokenize\n",
      "    tokenizer = PunktTokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/data.py\", line 582, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/home/nltk_data'\n",
      "    - '/opt/conda/miniconda3/nltk_data'\n",
      "    - '/opt/conda/miniconda3/share/nltk_data'\n",
      "    - '/opt/conda/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:181)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/04/19 10:59:20 ERROR TaskSetManager: Task 1 in stage 14.0 failed 4 times; aborting job\n",
      "25/04/19 10:59:21 WARN TaskSetManager: Lost task 0.3 in stage 14.0 (TID 322) (st446-cluster-w08-w-0.europe-west2-c.c.delta-container-448310-n4.internal executor 12): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 14.0 failed 4 times, most recent failure: Lost task 1.3 in stage 14.0 (TID 321) (st446-cluster-w08-w-1.europe-west2-c.c.delta-container-448310-n4.internal executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12453/2792152010.py\", line 6, in preprocess_text\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 129, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 106, in sent_tokenize\n",
      "    tokenizer = PunktTokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/data.py\", line 582, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/home/nltk_data'\n",
      "    - '/opt/conda/miniconda3/nltk_data'\n",
      "    - '/opt/conda/miniconda3/share/nltk_data'\n",
      "    - '/opt/conda/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:181)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/04/19 10:59:21 WARN TaskSetManager: Lost task 3.1 in stage 14.0 (TID 325) (st446-cluster-w08-w-1.europe-west2-c.c.delta-container-448310-n4.internal executor 14): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 14.0 failed 4 times, most recent failure: Lost task 1.3 in stage 14.0 (TID 321) (st446-cluster-w08-w-1.europe-west2-c.c.delta-container-448310-n4.internal executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12453/2792152010.py\", line 6, in preprocess_text\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 129, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 106, in sent_tokenize\n",
      "    tokenizer = PunktTokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/data.py\", line 582, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/home/nltk_data'\n",
      "    - '/opt/conda/miniconda3/nltk_data'\n",
      "    - '/opt/conda/miniconda3/share/nltk_data'\n",
      "    - '/opt/conda/miniconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:181)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "[Stage 14:>                                                         (0 + 2) / 5]\r"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_12453/2792152010.py\", line 6, in preprocess_text\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 129, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 106, in sent_tokenize\n    tokenizer = PunktTokenizer(language)\n                ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n    self.load_lang(lang)\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/data.py\", line 582, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/nltk_data'\n    - '/opt/conda/miniconda3/nltk_data'\n    - '/opt/conda/miniconda3/share/nltk_data'\n    - '/opt/conda/miniconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_change\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexplicit_class\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_12453/2792152010.py\", line 6, in preprocess_text\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 129, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\", line 106, in sent_tokenize\n    tokenizer = PunktTokenizer(language)\n                ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n    self.load_lang(lang)\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/nltk/data.py\", line 582, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/nltk_data'\n    - '/opt/conda/miniconda3/nltk_data'\n    - '/opt/conda/miniconda3/share/nltk_data'\n    - '/opt/conda/miniconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n"
     ]
    }
   ],
   "source": [
    "df_change.groupBy(\"explicit_class\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb57bca-7b5d-4983-92e1-29a41087924e",
   "metadata": {
    "collapsed": true,
    "id": "6fb57bca-7b5d-4983-92e1-29a41087924e",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1a822c7e-e7de-4eac-fbb6-84e5da067db1",
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: gs://dataproc-staging-europe-west2-869961533204-pghri6io/corpus-webis-tldr-17.json.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#given the massive size of our data this step is essential otherwise the data takes too long to load\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#the schema is given on the original website\u001b[39;00m\n\u001b[1;32m      4\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m      5\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      6\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m ])\n\u001b[0;32m---> 18\u001b[0m original_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://dataproc-staging-europe-west2-869961533204-pghri6io/corpus-webis-tldr-17.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m original_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:425\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(iterator: Iterable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: gs://dataproc-staging-europe-west2-869961533204-pghri6io/corpus-webis-tldr-17.json."
     ]
    }
   ],
   "source": [
    "#given the massive size of our data this step is essential otherwise the data takes too long to load\n",
    "#the schema is given on the original website\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"body\", StringType(), True),\n",
    "    StructField(\"normalizedBody\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"content_len\", LongType(), True),\n",
    "    StructField(\"summary\", StringType(), True),\n",
    "    StructField(\"summary_len\", LongType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"subreddit\", StringType(), True),\n",
    "    StructField(\"subreddit_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True)\n",
    "])\n",
    "\n",
    "original_df = spark.read.schema(schema).json(\"gs://dataproc-staging-europe-west2-869961533204-pghri6io/corpus-webis-tldr-17.json\")\n",
    "original_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c4ee8b-d8d6-4c34-abfb-be16aeedb920",
   "metadata": {
    "id": "83c4ee8b-d8d6-4c34-abfb-be16aeedb920",
    "outputId": "2e6a17ec-8e6d-467b-96b2-9d96776134cf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>(145 + 2) / 147]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (9212, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "shape = (df_cmview.count(), len(df_cmview.columns))\n",
    "print(f\"Shape: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de855b14-38ee-421a-829e-b09a030e54f8",
   "metadata": {
    "id": "de855b14-38ee-421a-829e-b09a030e54f8",
    "outputId": "0b5d40ac-50b9-4fcd-debe-faf6c544d38f",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:======================================================>(145 + 2) / 147]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw lines: 3848330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Curious to see the size the of the file, checking through a count approach is too long compared to rdd\n",
    "rdd_shape = spark.sparkContext.textFile(\"gs://dataproc-staging-europe-west2-869961533204-pghri6io/corpus-webis-tldr-17.json\")\n",
    "print(f\"Total raw lines: {rdd_shape.count()}\")\n",
    "\n",
    "#There are 3,848,330 observations in our original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1338a6-227c-4ad2-a2d9-0934ec534720",
   "metadata": {
    "collapsed": true,
    "id": "5c1338a6-227c-4ad2-a2d9-0934ec534720",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "adb91c6e-282d-4494-cbb1-bf5b3c4506a6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "|title|normalizedBody                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |subreddit   |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "|NULL |Some people like simple. When looking at different computers out today there is much variety in the windows department. Laptops not only range from $500-$1700 but so much of it is absolute crap. I was reading earlier about an HP computer where the motherboard had melted through. While this is a rare occasion windows computers require research to some extent. Mac is simple, while there is 5 options all 5 are fine computers. Also consider macs are pretty, they don't use any plastic and have a very nice polished look to them. Finally macs users have brand loyalty, my dad bought an apple TV for himself and that thing works so well, connect to WiFi and it instantly allows you to steam videos. \\n TL;DR: Reliability, Image, Brand loyalty. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |changemyview|\n",
      "|NULL |There's a lot of identity wrapped up in being a \"Mac Person\" or a \"PC Person\" or a \"Linux Person.\" \\n This isn't a mistake. Computers are now marketed the same way cars are. You buy a Camero because you're \"badass and I know it.\" You buy a Lexus because \"fuck off Proletarian swine!\" \\n Its the same marketing. \\n I use my Mac computers for work. I'm an audio engineer and I record musicians, do system optimization (make the PA sound good), and use the laptop for soft-synths. Mac's aren't perfect by any means, but they're stable. That's all I really care about when I have clients breathing down my neck. It does the job I need it to. \\n Its just a tool. Its not who I am. Its the right tool for the right job  for me . \\n I'm about to build a PC for gaming, because I need to constantly have a project and PC gaming kinda kicks ass. Honestly, if I didn't do the audio gig, I'd rock a PC all day long. I can check my email and look at cat pictures without a Mac. \\n TL;DR: Your computer is not who you are. They're just tools. Pick the right tool for the right job. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |changemyview|\n",
      "|NULL |I think legalizing gay marriage would be fixing a symptom, rather than curing a disease, so to speak. In other words, gay marriage is not the problem, gay marriage is caused by a bigger problem. That bigger problem if the US government violating its own principle of \"separation of church and state\" by giving special perks and benefits to those who have participated in a particular religious ordinance. \\n Gay marriage should not be legalized because it would just be spreading that problem. We need to take religion COMPLETELY out of state, not just where its inconvenient. \\n Gay marriage should not be illegal, either. The government should not have any say at all. It does not have that right. It would be like banning Bar Mitzvah's or the Hajj. It should not get a vote on the matter. Leave that up to individual churches if they want to allow it or not. \\n However, that would leave a huge gap in our society. How do we decide who gets to say if a hospital pulls the plug on you? How do we decide who gets your crap if you die? How do we decide how much less you should pay in taxes because you're supporting another adult at home? \\n It wouldn't be hard to, for example, allow each adult to designate one other adult, who must choose them in return, to make these decisions, share these benefits, etc, regardless of gender, sexual orientation, etc. \\n TL;DR - Legalizing gay marriage would not fix a problem. It would worsen a deeper problem. Marriage should not even have a legality status, and gender, race, age, sexuality, etc should not be a factor in who gets certain basic rights and privileges. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |changemyview|\n",
      "|NULL |my personal understanding has always been that, provided 2 explanations have equal explanatory power, the simpler one is to be preferred. \\nI understand this to be true because in science, we always start out by defining and understanding small parts of the whole, then slowly peace it together as time goes on, possibly adjusting our model along the way (shout out to lakatos!) \\n In explaining lightning, \"zeus\" is a theory. So is electric discharge. The latter is simpler, hence i prefer it, test it, find it to have some truth in it. Thus i can adjust my theory, and say \"lightning is, in some way, caused by electric discharge\" and then keep investigating that way. \\n should i find lightning not to be connected to electric discharge, i can always return to the more complicated option, zeus, and investigate in that direction. \\n tl;dr: occams razor is a suggestion for more effective theory building. \\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |changemyview|\n",
      "|NULL |When you present someone with an idea their brain will process it i.e. any information you give is at least involuntarily reacted to. If you draw attention to something negative subjectively or pseudo-objectively in that persons mind the next event is an emotional response. If you know that and have chosen to say something you have also chosen to illicit that response even if there is some probability involved here and it's much closer to a shared responsibility biased towards you. \\n An analogous process is changing someones emotional state by injecting psychoactive drugs into a persons bloodstream. If you do that, you would be responsible for changing someone's emotional state, right? (Excepting the admonished responsibility due to an individual tolerance/sensitivity/resistance etc.) If you consider that neurological connections can be broken or reinforced by verbal information and stimuli, i.e. associations with emotional states can be made via language alone, then if you possess in advance sets of information corresponding to a particular emotional state and use it, I honestly don't see it being particularly different to injecting drugs into someones body. \\n Essentially, I just can't accept your model of responsibility as being binary, it's a very complicated network. \\n Equally, I would actually argue the exact opposite of you and say you are much less responsible for your own emotional state than your environment is. Someone who is exposed to great depths of poor treatment and verbal abuse throughout their life will carry a much heavier emotional weight because of it, yet the responsibility for that state would not be the individual's. \\n Subscribing this to determinism is pointless (although obviously is the case) as the discussion is shifted from personal responsibility to responsibility is arbitrary in a single word, it's a macroscopic view and doesn't really help you discuss individual instances of responsibility. Considering responsibility in single interactions where both parties are essentially glorified turing machines and supposing responsibility would be attributed on the basis of one GTM reaching an offensive state causing GTM2 to reach an upset state, for me it still satisfies the condition that you can be held responsible for causing someone elses emotional distress even if the previous states were reached by other interactions. \\n Saying that being responsible for your emotional state because it's yours is the same kind of responsibility as owning a mobile phone, you're responsible for keeping it safe but if someone takes it out of your hands and throws it into a lake I would say it is their responsiblity for destroying it. \\n tldr : \\n \\n Determinism/Personal Responsibility not mutually exclusive. \\n Responibility is a network \\n You are responsible for your own emotions in the sense of ownership \\n Nigger is an awful word and I wouldn't get comfortable using it, even with friends \\n \\n|changemyview|\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cmview = original_df.filter(original_df[\"subreddit\"] == \"changemyview\")\n",
    "df_cmview.select(\"title\", \"normalizedBody\", \"subreddit\").show(5, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
